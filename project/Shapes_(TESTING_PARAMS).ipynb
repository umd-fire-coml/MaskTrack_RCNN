{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect_crop_image_by_mask.ipynb  re_id.py\n",
      "inspect_crop_image_by_mask.py\t  Shapes.ipynb\n",
      "inspect_wad_data.ipynb\t\t  Shapes_(TESTING_PARAMS).ipynb\n",
      "mask_propagation.py\t\t  wad_data_inference.ipynb\n",
      "mask_rcnn_coco.h5\t\t  wad_data.py\n",
      "mask_rcnn.egg-info\t\t  wad_data_training.ipynb\n",
      "__pycache__\n",
      "/home/shared/project/MaskTrack_RCNN/project\n"
     ]
    }
   ],
   "source": [
    "!ls\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"/home/shared/project/MaskTrack_RCNN\")\n",
    "os.chdir(ROOT_DIR)\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "### KEEPS DOWNLOADING MODEL WEIGHTS INTO DIFFERENT DIRECTORIES!\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assets\t images       mask_rcnn_coco.h5   project\t    samples\r\n",
      "build\t LICENSE      mask_rcnn.egg-info  pwc_net\t    setup.cfg\r\n",
      "dataset  logs\t      model\t\t  README.md\t    setup.py\r\n",
      "dist\t MANIFEST.in  mrcnn\t\t  requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     8\n",
      "BBOX_STD_DEV                   [ 0.1  0.1  0.2  0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 8\n",
      "IMAGE_MAX_DIM                  128\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  128\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              square\n",
      "IMAGE_SHAPE                    [128 128   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [ 123.7  116.8  103.9]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           shapes\n",
      "NUM_CLASSES                    4\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [ 0.1  0.1  0.2  0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           32\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ShapesConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"shapes\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # background + 3 shapes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "    BACKBONE = \"resnet101\"\n",
    "    \n",
    "    \n",
    "config = ShapesConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapesDataset(utils.Dataset):\n",
    "    \"\"\"Generates the shapes synthetic dataset. The dataset consists of simple\n",
    "    shapes (triangles, squares, circles) placed randomly on a blank surface.\n",
    "    The images are generated on the fly. No file access required.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_shapes(self, count, height, width):\n",
    "        \"\"\"Generate the requested number of synthetic images.\n",
    "        count: number of images to generate.\n",
    "        height, width: the size of the generated images.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"shapes\", 1, \"square\")\n",
    "        self.add_class(\"shapes\", 2, \"circle\")\n",
    "        self.add_class(\"shapes\", 3, \"triangle\")\n",
    "\n",
    "        # Add images\n",
    "        # Generate random specifications of images (i.e. color and\n",
    "        # list of shapes sizes and locations). This is more compact than\n",
    "        # actual images. Images are generated on the fly in load_image().\n",
    "        for i in range(count):\n",
    "            bg_color, shapes = self.random_image(height, width)\n",
    "            self.add_image(\"shapes\", image_id=i, path=None,\n",
    "                           width=width, height=height,\n",
    "                           bg_color=bg_color, shapes=shapes)\n",
    "\n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Generate an image from the specs of the given image ID.\n",
    "        Typically this function loads the image from a file, but\n",
    "        in this case it generates the image on the fly from the\n",
    "        specs in image_info.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        bg_color = np.array(info['bg_color']).reshape([1, 1, 3])\n",
    "        image = np.ones([info['height'], info['width'], 3], dtype=np.uint8)\n",
    "        image = image * bg_color.astype(np.uint8)\n",
    "        for shape, color, dims in info['shapes']:\n",
    "            image = self.draw_shape(image, shape, dims, color)\n",
    "        return image\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"shapes\":\n",
    "            return info[\"shapes\"]\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for shapes of the given image ID.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        shapes = info['shapes']\n",
    "        count = len(shapes)\n",
    "        mask = np.zeros([info['height'], info['width'], count], dtype=np.uint8)\n",
    "        for i, (shape, _, dims) in enumerate(info['shapes']):\n",
    "            mask[:, :, i:i+1] = self.draw_shape(mask[:, :, i:i+1].copy(),\n",
    "                                                shape, dims, 1)\n",
    "        # Handle occlusions\n",
    "        occlusion = np.logical_not(mask[:, :, -1]).astype(np.uint8)\n",
    "        for i in range(count-2, -1, -1):\n",
    "            mask[:, :, i] = mask[:, :, i] * occlusion\n",
    "            occlusion = np.logical_and(occlusion, np.logical_not(mask[:, :, i]))\n",
    "        # Map class names to class IDs.\n",
    "        class_ids = np.array([self.class_names.index(s[0]) for s in shapes])\n",
    "        return mask.astype(np.bool), class_ids.astype(np.int32)\n",
    "\n",
    "    def draw_shape(self, image, shape, dims, color):\n",
    "        \"\"\"Draws a shape from the given specs.\"\"\"\n",
    "        # Get the center x, y and the size s\n",
    "        x, y, s = dims\n",
    "        if shape == 'square':\n",
    "            cv2.rectangle(image, (x-s, y-s), (x+s, y+s), color, -1)\n",
    "        elif shape == \"circle\":\n",
    "            cv2.circle(image, (x, y), s, color, -1)\n",
    "        elif shape == \"triangle\":\n",
    "            points = np.array([[(x, y-s),\n",
    "                                (x-s/math.sin(math.radians(60)), y+s),\n",
    "                                (x+s/math.sin(math.radians(60)), y+s),\n",
    "                                ]], dtype=np.int32)\n",
    "            cv2.fillPoly(image, points, color)\n",
    "        return image\n",
    "\n",
    "    def random_shape(self, height, width):\n",
    "        \"\"\"Generates specifications of a random shape that lies within\n",
    "        the given height and width boundaries.\n",
    "        Returns a tuple of three valus:\n",
    "        * The shape name (square, circle, ...)\n",
    "        * Shape color: a tuple of 3 values, RGB.\n",
    "        * Shape dimensions: A tuple of values that define the shape size\n",
    "                            and location. Differs per shape type.\n",
    "        \"\"\"\n",
    "        # Shape\n",
    "        shape = random.choice([\"square\", \"circle\", \"triangle\"])\n",
    "        # Color\n",
    "        color = tuple([random.randint(0, 255) for _ in range(3)])\n",
    "        # Center x, y\n",
    "        buffer = 20\n",
    "        y = random.randint(buffer, height - buffer - 1)\n",
    "        x = random.randint(buffer, width - buffer - 1)\n",
    "        # Size\n",
    "        s = random.randint(buffer, height//4)\n",
    "        return shape, color, (x, y, s)\n",
    "\n",
    "    def random_image(self, height, width):\n",
    "        \"\"\"Creates random specifications of an image with multiple shapes.\n",
    "        Returns the background color of the image and a list of shape\n",
    "        specifications that can be used to draw the image.\n",
    "        \"\"\"\n",
    "        # Pick random background color\n",
    "        bg_color = np.array([random.randint(0, 255) for _ in range(3)])\n",
    "        # Generate a few random shapes and record their\n",
    "        # bounding boxes\n",
    "        shapes = []\n",
    "        boxes = []\n",
    "        N = random.randint(1, 1)\n",
    "        for _ in range(N):\n",
    "            shape, color, dims = self.random_shape(height, width)\n",
    "            shapes.append((shape, color, dims))\n",
    "            x, y, s = dims\n",
    "            boxes.append([y-s, x-s, y+s, x+s])\n",
    "        # Apply non-max suppression wit 0.3 threshold to avoid\n",
    "        # shapes covering each other\n",
    "        keep_ixs = utils.non_max_suppression(np.array(boxes), np.arange(N), 0.3)\n",
    "        shapes = [s for i, s in enumerate(shapes) if i in keep_ixs]\n",
    "        return bg_color, shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ShapesDataset()\n",
    "dataset_train.load_shapes(500, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ShapesDataset()\n",
    "dataset_val.load_shapes(50, config.IMAGE_SHAPE[0], config.IMAGE_SHAPE[1])\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 0. LR=0.01\n",
      "\n",
      "Checkpoint Path: /home/shared/project/MaskTrack_RCNN/logs/shapes20180627T1437/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2348: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 11s 11s/step - loss: 7.9739 - rpn_class_loss: 0.0543 - rpn_bbox_loss: 3.3611 - mrcnn_class_loss: 1.8538 - mrcnn_bbox_loss: 1.2016 - mrcnn_mask_loss: 1.5032 - val_loss: 5.4464 - val_rpn_class_loss: 0.0476 - val_rpn_bbox_loss: 2.1983 - val_mrcnn_class_loss: 0.8205 - val_mrcnn_bbox_loss: 1.0931 - val_mrcnn_mask_loss: 1.2869\n",
      "Head Training: 44.859571005\n"
     ]
    }
   ],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "hstart_time = 0\n",
    "hstart_time = time.process_time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE * 10, \n",
    "            epochs=1, \n",
    "            layers='heads')\n",
    "head_time = time.process_time() - hstart_time\n",
    "print(\"Head Training: \" + str(head_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_path = model.find_last()\n",
    "#model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting at epoch 1. LR=0.001\n",
      "\n",
      "Checkpoint Path: /home/shared/project/MaskTrack_RCNN/logs/shapes20180627T1437/mask_rcnn_shapes_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "conv1                  (Conv2D)\n",
      "bn_conv1               (BatchNorm)\n",
      "res2a_branch2a         (Conv2D)\n",
      "bn2a_branch2a          (BatchNorm)\n",
      "res2a_branch2b         (Conv2D)\n",
      "bn2a_branch2b          (BatchNorm)\n",
      "res2a_branch2c         (Conv2D)\n",
      "res2a_branch1          (Conv2D)\n",
      "bn2a_branch2c          (BatchNorm)\n",
      "bn2a_branch1           (BatchNorm)\n",
      "res2b_branch2a         (Conv2D)\n",
      "bn2b_branch2a          (BatchNorm)\n",
      "res2b_branch2b         (Conv2D)\n",
      "bn2b_branch2b          (BatchNorm)\n",
      "res2b_branch2c         (Conv2D)\n",
      "bn2b_branch2c          (BatchNorm)\n",
      "res2c_branch2a         (Conv2D)\n",
      "bn2c_branch2a          (BatchNorm)\n",
      "res2c_branch2b         (Conv2D)\n",
      "bn2c_branch2b          (BatchNorm)\n",
      "res2c_branch2c         (Conv2D)\n",
      "bn2c_branch2c          (BatchNorm)\n",
      "res3a_branch2a         (Conv2D)\n",
      "bn3a_branch2a          (BatchNorm)\n",
      "res3a_branch2b         (Conv2D)\n",
      "bn3a_branch2b          (BatchNorm)\n",
      "res3a_branch2c         (Conv2D)\n",
      "res3a_branch1          (Conv2D)\n",
      "bn3a_branch2c          (BatchNorm)\n",
      "bn3a_branch1           (BatchNorm)\n",
      "res3b_branch2a         (Conv2D)\n",
      "bn3b_branch2a          (BatchNorm)\n",
      "res3b_branch2b         (Conv2D)\n",
      "bn3b_branch2b          (BatchNorm)\n",
      "res3b_branch2c         (Conv2D)\n",
      "bn3b_branch2c          (BatchNorm)\n",
      "res3c_branch2a         (Conv2D)\n",
      "bn3c_branch2a          (BatchNorm)\n",
      "res3c_branch2b         (Conv2D)\n",
      "bn3c_branch2b          (BatchNorm)\n",
      "res3c_branch2c         (Conv2D)\n",
      "bn3c_branch2c          (BatchNorm)\n",
      "res3d_branch2a         (Conv2D)\n",
      "bn3d_branch2a          (BatchNorm)\n",
      "res3d_branch2b         (Conv2D)\n",
      "bn3d_branch2b          (BatchNorm)\n",
      "res3d_branch2c         (Conv2D)\n",
      "bn3d_branch2c          (BatchNorm)\n",
      "res4a_branch2a         (Conv2D)\n",
      "bn4a_branch2a          (BatchNorm)\n",
      "res4a_branch2b         (Conv2D)\n",
      "bn4a_branch2b          (BatchNorm)\n",
      "res4a_branch2c         (Conv2D)\n",
      "res4a_branch1          (Conv2D)\n",
      "bn4a_branch2c          (BatchNorm)\n",
      "bn4a_branch1           (BatchNorm)\n",
      "res4b_branch2a         (Conv2D)\n",
      "bn4b_branch2a          (BatchNorm)\n",
      "res4b_branch2b         (Conv2D)\n",
      "bn4b_branch2b          (BatchNorm)\n",
      "res4b_branch2c         (Conv2D)\n",
      "bn4b_branch2c          (BatchNorm)\n",
      "res4c_branch2a         (Conv2D)\n",
      "bn4c_branch2a          (BatchNorm)\n",
      "res4c_branch2b         (Conv2D)\n",
      "bn4c_branch2b          (BatchNorm)\n",
      "res4c_branch2c         (Conv2D)\n",
      "bn4c_branch2c          (BatchNorm)\n",
      "res4d_branch2a         (Conv2D)\n",
      "bn4d_branch2a          (BatchNorm)\n",
      "res4d_branch2b         (Conv2D)\n",
      "bn4d_branch2b          (BatchNorm)\n",
      "res4d_branch2c         (Conv2D)\n",
      "bn4d_branch2c          (BatchNorm)\n",
      "res4e_branch2a         (Conv2D)\n",
      "bn4e_branch2a          (BatchNorm)\n",
      "res4e_branch2b         (Conv2D)\n",
      "bn4e_branch2b          (BatchNorm)\n",
      "res4e_branch2c         (Conv2D)\n",
      "bn4e_branch2c          (BatchNorm)\n",
      "res4f_branch2a         (Conv2D)\n",
      "bn4f_branch2a          (BatchNorm)\n",
      "res4f_branch2b         (Conv2D)\n",
      "bn4f_branch2b          (BatchNorm)\n",
      "res4f_branch2c         (Conv2D)\n",
      "bn4f_branch2c          (BatchNorm)\n",
      "res4g_branch2a         (Conv2D)\n",
      "bn4g_branch2a          (BatchNorm)\n",
      "res4g_branch2b         (Conv2D)\n",
      "bn4g_branch2b          (BatchNorm)\n",
      "res4g_branch2c         (Conv2D)\n",
      "bn4g_branch2c          (BatchNorm)\n",
      "res4h_branch2a         (Conv2D)\n",
      "bn4h_branch2a          (BatchNorm)\n",
      "res4h_branch2b         (Conv2D)\n",
      "bn4h_branch2b          (BatchNorm)\n",
      "res4h_branch2c         (Conv2D)\n",
      "bn4h_branch2c          (BatchNorm)\n",
      "res4i_branch2a         (Conv2D)\n",
      "bn4i_branch2a          (BatchNorm)\n",
      "res4i_branch2b         (Conv2D)\n",
      "bn4i_branch2b          (BatchNorm)\n",
      "res4i_branch2c         (Conv2D)\n",
      "bn4i_branch2c          (BatchNorm)\n",
      "res4j_branch2a         (Conv2D)\n",
      "bn4j_branch2a          (BatchNorm)\n",
      "res4j_branch2b         (Conv2D)\n",
      "bn4j_branch2b          (BatchNorm)\n",
      "res4j_branch2c         (Conv2D)\n",
      "bn4j_branch2c          (BatchNorm)\n",
      "res4k_branch2a         (Conv2D)\n",
      "bn4k_branch2a          (BatchNorm)\n",
      "res4k_branch2b         (Conv2D)\n",
      "bn4k_branch2b          (BatchNorm)\n",
      "res4k_branch2c         (Conv2D)\n",
      "bn4k_branch2c          (BatchNorm)\n",
      "res4l_branch2a         (Conv2D)\n",
      "bn4l_branch2a          (BatchNorm)\n",
      "res4l_branch2b         (Conv2D)\n",
      "bn4l_branch2b          (BatchNorm)\n",
      "res4l_branch2c         (Conv2D)\n",
      "bn4l_branch2c          (BatchNorm)\n",
      "res4m_branch2a         (Conv2D)\n",
      "bn4m_branch2a          (BatchNorm)\n",
      "res4m_branch2b         (Conv2D)\n",
      "bn4m_branch2b          (BatchNorm)\n",
      "res4m_branch2c         (Conv2D)\n",
      "bn4m_branch2c          (BatchNorm)\n",
      "res4n_branch2a         (Conv2D)\n",
      "bn4n_branch2a          (BatchNorm)\n",
      "res4n_branch2b         (Conv2D)\n",
      "bn4n_branch2b          (BatchNorm)\n",
      "res4n_branch2c         (Conv2D)\n",
      "bn4n_branch2c          (BatchNorm)\n",
      "res4o_branch2a         (Conv2D)\n",
      "bn4o_branch2a          (BatchNorm)\n",
      "res4o_branch2b         (Conv2D)\n",
      "bn4o_branch2b          (BatchNorm)\n",
      "res4o_branch2c         (Conv2D)\n",
      "bn4o_branch2c          (BatchNorm)\n",
      "res4p_branch2a         (Conv2D)\n",
      "bn4p_branch2a          (BatchNorm)\n",
      "res4p_branch2b         (Conv2D)\n",
      "bn4p_branch2b          (BatchNorm)\n",
      "res4p_branch2c         (Conv2D)\n",
      "bn4p_branch2c          (BatchNorm)\n",
      "res4q_branch2a         (Conv2D)\n",
      "bn4q_branch2a          (BatchNorm)\n",
      "res4q_branch2b         (Conv2D)\n",
      "bn4q_branch2b          (BatchNorm)\n",
      "res4q_branch2c         (Conv2D)\n",
      "bn4q_branch2c          (BatchNorm)\n",
      "res4r_branch2a         (Conv2D)\n",
      "bn4r_branch2a          (BatchNorm)\n",
      "res4r_branch2b         (Conv2D)\n",
      "bn4r_branch2b          (BatchNorm)\n",
      "res4r_branch2c         (Conv2D)\n",
      "bn4r_branch2c          (BatchNorm)\n",
      "res4s_branch2a         (Conv2D)\n",
      "bn4s_branch2a          (BatchNorm)\n",
      "res4s_branch2b         (Conv2D)\n",
      "bn4s_branch2b          (BatchNorm)\n",
      "res4s_branch2c         (Conv2D)\n",
      "bn4s_branch2c          (BatchNorm)\n",
      "res4t_branch2a         (Conv2D)\n",
      "bn4t_branch2a          (BatchNorm)\n",
      "res4t_branch2b         (Conv2D)\n",
      "bn4t_branch2b          (BatchNorm)\n",
      "res4t_branch2c         (Conv2D)\n",
      "bn4t_branch2c          (BatchNorm)\n",
      "res4u_branch2a         (Conv2D)\n",
      "bn4u_branch2a          (BatchNorm)\n",
      "res4u_branch2b         (Conv2D)\n",
      "bn4u_branch2b          (BatchNorm)\n",
      "res4u_branch2c         (Conv2D)\n",
      "bn4u_branch2c          (BatchNorm)\n",
      "res4v_branch2a         (Conv2D)\n",
      "bn4v_branch2a          (BatchNorm)\n",
      "res4v_branch2b         (Conv2D)\n",
      "bn4v_branch2b          (BatchNorm)\n",
      "res4v_branch2c         (Conv2D)\n",
      "bn4v_branch2c          (BatchNorm)\n",
      "res4w_branch2a         (Conv2D)\n",
      "bn4w_branch2a          (BatchNorm)\n",
      "res4w_branch2b         (Conv2D)\n",
      "bn4w_branch2b          (BatchNorm)\n",
      "res4w_branch2c         (Conv2D)\n",
      "bn4w_branch2c          (BatchNorm)\n",
      "res5a_branch2a         (Conv2D)\n",
      "bn5a_branch2a          (BatchNorm)\n",
      "res5a_branch2b         (Conv2D)\n",
      "bn5a_branch2b          (BatchNorm)\n",
      "res5a_branch2c         (Conv2D)\n",
      "res5a_branch1          (Conv2D)\n",
      "bn5a_branch2c          (BatchNorm)\n",
      "bn5a_branch1           (BatchNorm)\n",
      "res5b_branch2a         (Conv2D)\n",
      "bn5b_branch2a          (BatchNorm)\n",
      "res5b_branch2b         (Conv2D)\n",
      "bn5b_branch2b          (BatchNorm)\n",
      "res5b_branch2c         (Conv2D)\n",
      "bn5b_branch2c          (BatchNorm)\n",
      "res5c_branch2a         (Conv2D)\n",
      "bn5c_branch2a          (BatchNorm)\n",
      "res5c_branch2b         (Conv2D)\n",
      "bn5c_branch2b          (BatchNorm)\n",
      "res5c_branch2c         (Conv2D)\n",
      "bn5c_branch2c          (BatchNorm)\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:98: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shared/anaconda3/lib/python3.6/site-packages/keras/engine/training.py:2348: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 18s 18s/step - loss: 4.0733 - rpn_class_loss: 0.0380 - rpn_bbox_loss: 1.5763 - mrcnn_class_loss: 0.3577 - mrcnn_bbox_loss: 1.2551 - mrcnn_mask_loss: 0.8461 - val_loss: 4.9300 - val_rpn_class_loss: 0.0444 - val_rpn_bbox_loss: 1.8211 - val_mrcnn_class_loss: 0.9002 - val_mrcnn_bbox_loss: 1.0493 - val_mrcnn_mask_loss: 1.1150\n",
      "All Training: 100.529054025\n"
     ]
    }
   ],
   "source": [
    "astart_time = 0\n",
    "astart_time = time.process_time()\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=2, \n",
    "            layers='all')\n",
    "all_time = head_time + time.process_time() - astart_time\n",
    "print(\"All Training: \" + str(all_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes(test).h5\")\n",
    "#model_path = model.find_last()\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from  /home/shared/project/MaskTrack_RCNN/logs/mask_rcnn_shapes(test).h5\n"
     ]
    }
   ],
   "source": [
    "class InferenceConfig(ShapesConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes(test).h5\")\n",
    "#model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original_image           shape: (128, 128, 3)         min:  101.00000  max:  209.00000  uint8\n",
      "image_meta               shape: (16,)                 min:    0.00000  max:  128.00000  int64\n",
      "gt_class_id              shape: (1,)                  min:    1.00000  max:    1.00000  int32\n",
      "gt_bbox                  shape: (1, 4)                min:   59.00000  max:  123.00000  int32\n",
      "gt_mask                  shape: (128, 128, 1)         min:    0.00000  max:    1.00000  bool\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1 images\n",
      "image                    shape: (128, 128, 3)         min:  101.00000  max:  209.00000  uint8\n",
      "molded_images            shape: (1, 128, 128, 3)      min:  -22.70000  max:   92.20000  float64\n",
      "image_metas              shape: (1, 16)               min:    0.00000  max:  128.00000  int64\n",
      "anchors                  shape: (1, 4092, 4)          min:   -0.71267  max:    1.20874  float32\n",
      "\n",
      "*** No instances to display *** \n",
      "\n",
      "Head Training: 44.859571005\n",
      "All Training: 100.529054025\n",
      "Testing Time: 0.06781144000001404\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tstart_time = 0\n",
    "tstart_time = time.process_time()\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())\n",
    "testing_time = time.process_time() - tstart_time\n",
    "\n",
    "\n",
    "print(\"Head Training: \" + str(head_time))\n",
    "print(\"All Training: \" + str(all_time))\n",
    "print(\"Testing Time: \" + str(testing_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-a24f7a3ecbe7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Compute AP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     AP, precisions, recalls, overlaps =        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n\u001b[0;32m---> 15\u001b[0;31m                          r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mAPs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/MaskTrack_RCNN/mrcnn/utils.py\u001b[0m in \u001b[0;36mcompute_ap\u001b[0;34m(gt_boxes, gt_class_ids, gt_masks, pred_boxes, pred_class_ids, pred_scores, pred_masks, iou_threshold)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0mgt_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0mpred_boxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_class_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m         iou_threshold)\n\u001b[0m\u001b[1;32m    780\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[0;31m# Compute precision and recall at each prediction box step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/MaskTrack_RCNN/mrcnn/utils.py\u001b[0m in \u001b[0;36mcompute_matches\u001b[0;34m(gt_boxes, gt_class_ids, gt_masks, pred_boxes, pred_class_ids, pred_scores, pred_masks, iou_threshold, score_threshold)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m     \u001b[0;31m# Compute IoU overlaps [pred_masks, gt_masks]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 731\u001b[0;31m     \u001b[0moverlaps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_overlaps_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_masks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m     \u001b[0;31m# Loop through predictions and find matching ground truth boxes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/project/MaskTrack_RCNN/mrcnn/utils.py\u001b[0m in \u001b[0;36mcompute_overlaps_masks\u001b[0;34m(masks1, masks2)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# flatten masks and compute their areas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m     \u001b[0mmasks1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks1\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m     \u001b[0mmasks2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks2\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0marea1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mreshape\u001b[0;34m(a, newshape, order)\u001b[0m\n\u001b[1;32m    230\u001b[0m            [5, 6]])\n\u001b[1;32m    231\u001b[0m     \"\"\"\n\u001b[0;32m--> 232\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reshape'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# An AttributeError occurs if the object does not have\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (0)"
     ]
    }
   ],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
